{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Causal Knowledge Transfer for Safe Reinforcement Learning",
   "id": "9b3534bf33e6bed9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train Agents\n",
    "#### Sumo Network File\n",
    "When editing the sumo network (nets/simple_unprotected_right.net.xml) never edit the xml directly. Instead, go to nets/netconfig and make desired changes there. Generate the new net.xml by executing generate config.sh\n",
    "#### Sumo Route File\n",
    "This is part of the generate_config.sh now.\n",
    "#### Reward Function\n",
    "* TODO: Come up with a fitting reward function that penalises collisions\n",
    "#### RL Training\n",
    "* TODO: Come up with Hyperparameters for the training loop\n",
    "\n",
    "**Desired Output: Trained_Model.zip**"
   ],
   "id": "76a3883d1cec1ee8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creating the Sumo Environment",
   "id": "696c12a4c9cece3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from env.SumoEnvironmentGenerator import SumoEnvironmentGenerator\n",
    "from pathlib import Path\n",
    "\n",
    "environments = SumoEnvironmentGenerator(\n",
    "    net_file=str(Path().joinpath('nets', 'simple_unprotected_right', 'simple_unprotected_right.net.xml')),\n",
    "    route_file=str(Path().joinpath('nets', 'simple_unprotected_right', 'simple_unprotected_right.rou.xml')),\n",
    "    sumocfg_file=str(Path().joinpath('nets', 'simple_unprotected_right', 'simple_unprotected_right.sumocfg')),\n",
    "    duration=3600,\n",
    "    learning_data_csv_name=str(Path().joinpath('env', 'training_data', 'output.csv')),\n",
    ")"
   ],
   "id": "83958ce226f27b73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training and saving the Model",
   "id": "e6f9f4917eb5beff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from stable_baselines3.dqn import DQN\n",
    "\n",
    "%load_ext tensorboard\n",
    "env = environments.get_training_env()\n",
    "model = DQN(\n",
    "    env=environments.get_training_env(),\n",
    "    policy='MlpPolicy',\n",
    "    learning_rate=0.001,\n",
    "    learning_starts=0,\n",
    "    train_freq=1,\n",
    "    target_update_interval=500,\n",
    "    exploration_fraction=0.05,\n",
    "    exploration_final_eps=0.01,\n",
    "    verbose=1,\n",
    "    tensorboard_log='dqn_sumo_tensorboard'\n",
    ")\n",
    "model.learn(10_000, tb_log_name='test_run_short')\n",
    "model.save(Path().joinpath('env', 'training_data', 'dqn'))"
   ],
   "id": "8c3f5ee8509cdf52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Giving the model a test run in an evaluation environment",
   "id": "4604104a64fb90ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from stable_baselines3.dqn import DQN\n",
    "\n",
    "env = environments.get_demonstration_env()\n",
    "model = DQN(env=env, policy='MlpPolicy').load(Path().joinpath('env', 'training_data', 'dqn'))\n",
    "\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "env.close()"
   ],
   "id": "4e7f9cc33d31421c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Produce Traces\n",
    "Run the simulation repeatably to produce traces (data) for causal discovery.\n",
    "\n",
    "#### Data Selection\n",
    "TODO: Select which columns we want to do Causal Discovery on\n",
    "#### Data Summary\n",
    "TODO: Incorporate old data summary script\n",
    "\n",
    "**Desired Output: One CSV File containing all interesting data**\n"
   ],
   "id": "4f83a5026c16fe31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from stable_baselines3.dqn import DQN\n",
    "\n",
    "simulation_output_path = Path().joinpath('data', 'dqn')\n",
    "Path.mkdir(simulation_output_path, parents=True, exist_ok=True)\n",
    "\n",
    "for experiment in range(10):\n",
    "    env = environments.get_generation_env(output_prefix=str(simulation_output_path.joinpath(str(experiment).zfill(4))))\n",
    "    model = DQN(env=env, policy='MlpPolicy').load(Path().joinpath('env', 'training_data', 'dqn'))\n",
    "    \n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "    env.close()"
   ],
   "id": "b973182d439275c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Causal Discovery\n",
    "Discover causal graph\n",
    "\n",
    "TODO: Decide which discovery algorithm to use\n",
    "\n",
    "TODO: Figure out how to incorporate R code in Jupyter Notebook\n",
    "\n",
    "**Desired Output: Causal Graph XML File**"
   ],
   "id": "1de43ea98a1365db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO",
   "id": "ca7bf3cc83299253",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Fit MLMs\n",
    "Fit MLMs based on Causal Discovery Graph\n",
    "\n",
    "TODO: Parse Graph XML into MLM parameters / formulae\n",
    "\n",
    "**Desired Output: MLM**"
   ],
   "id": "a3d03cf3105ec494"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO",
   "id": "829786814c4f2bda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Produce Interventions\n",
    "\n",
    "#### Covariate Shift Distribution\n",
    "* Create a distribution for the covariate (friction) shift\n",
    "* Sample from distribution\n",
    "    * Fulfill Assumption: sparse sample data is representative for covariate shift ground truth\n",
    "* Produce Traces for sparse input data\n",
    "\n",
    "#### Crank MLM the other way\n",
    "* Calculate Intervention Distribution by inputting sparse data into MLM\n",
    "\n",
    "**Desired Output: Intervention Distribution**"
   ],
   "id": "a5c99e021c7da84f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO",
   "id": "32459df33ac2d588",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generate Posterior Distributions\n",
    "TODO: Generate Posterior Distributions without intervention\n",
    "\n",
    "TODO: Generate Posterior Distributions with intervention\n",
    "\n",
    "**Desired Output: Two XML Files**"
   ],
   "id": "c86f426933c2843a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO",
   "id": "730155056916357a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Query\n",
    "Compare Distributions and decide, which part of the model to retrain.\n",
    "\n",
    "TODO: Classify the data / model in parts"
   ],
   "id": "71229ba166fe756f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO",
   "id": "73190c3130a276a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluation\n",
    "\n",
    "#### Agent\n",
    "compare new resulting agent (partially continued training depending on Query) to:\n",
    "* Old agent (Lower performance bound)\n",
    "* Completely newly trained agent (upper performance bound)\n",
    "* (New Agent that is trained completely on new data (without Query))\n",
    "\n",
    "#### Intervention\n",
    "Function: Number of Covariate Shift Samples --> Wasserstein distance: Intervention vs. ground truth (distribution)\n",
    "\n",
    "#### MLM\n",
    "* Wasserstein Distance: Effect of Intervention vs. ground truth effect\n",
    "* Maybe also as a function of the number of retrain samples\n",
    "\n"
   ],
   "id": "6cdb163f340d239c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO",
   "id": "b28fb56662cf1a7a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
